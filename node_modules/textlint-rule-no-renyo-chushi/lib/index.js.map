{"version":3,"sources":["../src/index.js"],"names":["module","exports","context","options","Syntax","RuleError","report","getSource","Str","node","text","sentences","checkRenyo","a","b","errorPattern","token1","pos","pos_detail_1","conjugated_form","push","token2","surface_form","target","aToken1","bToken2","JSON","stringify","then","forEach","sentence","i","tokens","tokenizer","tokenizeForSentence","raw","token","j","length","result","regex","RegExp","indexOfError","exec","ruleError","index"],"mappings":"AAAA;;AACA;;AACA;;;;AAEAA,OAAOC,OAAP,GAAiB,UAASC,OAAT,EAAgC;AAAA,QAAdC,OAAc,uEAAJ,EAAI;AAAA,QACtCC,MADsC,GACEF,OADF,CACtCE,MADsC;AAAA,QAC9BC,SAD8B,GACEH,OADF,CAC9BG,SAD8B;AAAA,QACnBC,MADmB,GACEJ,OADF,CACnBI,MADmB;AAAA,QACXC,SADW,GACEL,OADF,CACXK,SADW;;AAE7C,+BACKH,OAAOI,GADZ,YACiBC,IADjB,EACsB;AACd,YAAMC,OAAOH,UAAUE,IAAV,CAAb;AACA,YAAME,YAAY,6BAAMD,IAAN,CAAlB;;AAEA,YAAME,aAAa,SAAbA,UAAa,CAACC,CAAD,EAAGC,CAAH,EAAS;;AAExB,gBAAMC,eAAe,EAArB;AACA,gBAAMC,SAAS,EAAf;AACAA,mBAAOC,GAAP,GAAa,IAAb;AACAD,mBAAOE,YAAP,GAAsB,IAAtB;AACAF,mBAAOG,eAAP,GAAyB,KAAzB;AACAJ,yBAAaK,IAAb,CAAkBJ,MAAlB;;AAEA,gBAAMK,SAAS,EAAf;AACAA,mBAAOC,YAAP,GAAsB,GAAtB;AACAD,mBAAOJ,GAAP,GAAa,IAAb;AACAF,yBAAaK,IAAb,CAAkBC,MAAlB;;AAEA,gBAAME,SAAS,EAAf;AACA,gBAAMC,UAAU,EAAhB;AACAA,oBAAQP,GAAR,GAAcJ,EAAEI,GAAhB;AACAO,oBAAQN,YAAR,GAAuBL,EAAEK,YAAzB;AACAM,oBAAQL,eAAR,GAA0BN,EAAEM,eAA5B;AACAI,mBAAOH,IAAP,CAAYI,OAAZ;;AAEA,gBAAMC,UAAU,EAAhB;AACAA,oBAAQH,YAAR,GAAuBR,EAAEQ,YAAzB;AACAG,oBAAQR,GAAR,GAAcH,EAAEG,GAAhB;AACAM,mBAAOH,IAAP,CAAYK,OAAZ;;AAEA,gBAAIC,KAAKC,SAAL,CAAeZ,YAAf,MAAiCW,KAAKC,SAAL,CAAeJ,MAAf,CAArC,EAA4D;AACxD,uBAAO,IAAP;AACH,aAFD,MAEO;AACH,uBAAO,KAAP;AACH;AAEJ,SAhCD;;AAkCA,eAAO,+BAAeK,IAAf,CAAoB,qBAAa;AACpCjB,sBAAUkB,OAAV,CAAkB,UAAUC,QAAV,EAAmBC,CAAnB,EAAsB;AACpC,oBAAMC,SAASC,UAAUC,mBAAV,CAA8BJ,SAASK,GAAvC,CAAf;;AAEIH,uBAAOH,OAAP,CAAe,UAASO,KAAT,EAAeC,CAAf,EAAkB;AAC7B;AACA,wBAAKA,IAAIL,OAAOM,MAAP,GAAgB,CAAzB,EAA4B;AACxB,4BAAMC,SAAS3B,WAAWoB,OAAOK,CAAP,CAAX,EAAqBL,OAAOK,IAAE,CAAT,CAArB,CAAf;;AAEA,4BAAKE,WAAW,IAAhB,EAAsB;AAClB,gCAAMC,QAAQ,IAAIC,MAAJ,CAAWT,OAAOK,CAAP,EAAUf,YAAV,GAAyBU,OAAOK,IAAE,CAAT,EAAYf,YAAhD,EAA8D,GAA9D,CAAd;AACA,gCAAMoB,eAAeF,MAAMG,IAAN,CAAWjC,IAAX,CAArB;AACA,gCAAMkC,YAAY,IAAIvC,SAAJ,CAAc,qBAAqB2B,OAAOK,CAAP,EAAUf,YAA/B,GAA8CU,OAAOK,IAAE,CAAT,EAAYf,YAAxE,EAAsF;AACpGuB,uCAAOH,aAAaG,KADgF,CAC3E;AAD2E,6BAAtF,CAAlB;AAGAvC,mCAAOG,IAAP,EAAamC,SAAb;AACH;AACJ;AACJ,iBAdD;AAeP,aAlBD;AAmBH,SApBM,CAAP;AAqBH,KA5DL;AA8DH,CAhED","file":"index.js","sourcesContent":["\"use strict\";\nimport {split} from \"sentence-splitter\";\nimport {getTokenizer} from \"kuromojin\";\n\nmodule.exports = function(context, options = {}) {\n    const {Syntax, RuleError, report, getSource} = context;\n    return {\n        [Syntax.Str](node){\n            const text = getSource(node);\n            const sentences = split(text);\n\n            const checkRenyo = (a,b) => {\n\n                const errorPattern = [];\n                const token1 = {};\n                token1.pos = '動詞';\n                token1.pos_detail_1 = '自立';\n                token1.conjugated_form = '連用形';\n                errorPattern.push(token1);\n\n                const token2 = {};\n                token2.surface_form = '、';\n                token2.pos = '名詞';\n                errorPattern.push(token2);\n                \n                const target = [];\n                const aToken1 = {};\n                aToken1.pos = a.pos;\n                aToken1.pos_detail_1 = a.pos_detail_1;\n                aToken1.conjugated_form = a.conjugated_form;\n                target.push(aToken1);\n                \n                const bToken2 = {};\n                bToken2.surface_form = b.surface_form;\n                bToken2.pos = b.pos;\n                target.push(bToken2);\n                \n                if (JSON.stringify(errorPattern) === JSON.stringify(target)){\n                    return true;\n                } else {\n                    return false;\n                }\n                \n            };\n\n            return getTokenizer().then(tokenizer => {\n                sentences.forEach(function (sentence,i) {\n                    const tokens = tokenizer.tokenizeForSentence(sentence.raw);\n\n                        tokens.forEach(function(token,j) {\n                            //　対象のtokenが文末でなければ確認する \n                            if ( j < tokens.length - 1 ){\n                                const result = checkRenyo(tokens[j],tokens[j+1]);\n                                \n                                if ( result === true ){\n                                    const regex = new RegExp(tokens[j].surface_form + tokens[j+1].surface_form, 'g');\n                                    const indexOfError = regex.exec(text);\n                                    const ruleError = new RuleError(\"連用中止形が使われています。: \" + tokens[j].surface_form + tokens[j+1].surface_form, {\n                                        index: indexOfError.index// padding of index\n                                    });\n                                    report(node, ruleError);\n                                }\n                            }\n                        });\n                });\n            });\n        }\n    };\n};\n"]}